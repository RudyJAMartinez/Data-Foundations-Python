{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (1 points)\n",
    "\n",
    "The files \"positive_words.txt\" and \"negative_words.txt\" contain mannually curated positive (e.g., good, great, awesome) and negative words (e.g., bad, hate, terrible). The files contain one word on each line. Write a function that takes the open file and adds the words (i.e., each line) to a set then returns it.\n",
    "\n",
    "**Note:** You should use \".strip()\" to remove the newline character from the end of each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n",
    "\n",
    "def file_to_set(file):\n",
    "    \"\"\"\n",
    "    This function should take a file handler as input and return a set.\n",
    "    \n",
    "        Parameters:\n",
    "            - file file handle: This variable is a file handle\n",
    "            \n",
    "        Return:\n",
    "            - The file should return a set (e.g., {'good', 'great', 'happy'})\n",
    "    \"\"\"\n",
    "    word_set = set()\n",
    "    \n",
    "    for word in file:\n",
    "        word_set.add(word.strip())\n",
    "\n",
    "    return word_set # You should return a set\n",
    "\n",
    "positive_file = open('./bing_liu/positive-words.txt', encoding='utf8')\n",
    "positive_words = file_to_set(positive_file)\n",
    "positive_file.close()\n",
    "\n",
    "negative_file = open('./bing_liu/negative-words.txt', encoding='iso-8859-1') # If you get a weird read error. Let me know. We can change the encoding.\n",
    "negative_words = file_to_set(negative_file)\n",
    "negative_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts finished successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(type(positive_words) == type(set()))\n",
    "assert(type(negative_words) == type(set()))\n",
    "assert(len(positive_words) == 2006)\n",
    "assert(len(negative_words) == 4783)\n",
    "assert(('good' in positive_words)  == True)\n",
    "assert(('bad' in negative_words)  == True)\n",
    "assert(('bad' not in positive_words) == True)\n",
    "print(\"Asserts finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (1 points)\n",
    "\n",
    "For this exercise, you need to write a function that counts the number of words in a sentence that also appear in a set. For example, given the set set(['good', 'great']) and the sentence \"this is good good good\", the function should return 3.\n",
    "\n",
    "**Hint:** You can check if something is in a set using the following notation:\n",
    "\n",
    "```python\n",
    "mySet = set([\"a\", \"b\", \"c\"])\n",
    "otherList = [\"c\", \"d\"]\n",
    "for letter in otherList:\n",
    "    if letter in mySet:\n",
    "        print(letter)\n",
    "```\n",
    "\n",
    "The above code will print \"c\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sentiment_words(sentiment_set, tweet_text, lower):\n",
    "    \"\"\"\n",
    "    This function takes a set and string as input, then counts the number of words that \n",
    "    appear in the string (tweet_text) that are also in the set (sentiment_set). The\n",
    "    tweet_text should be normalized based on the lower argument (i.e., lowercase if True)\n",
    "    \n",
    "        Parameters:\n",
    "            - sentiment_set set: A set of sentiment words, e.g., {'good', 'great', 'happy'}\n",
    "            - tweet_text string: A tweet, e.g., \"I go to UTSA!!!\"\n",
    "            - lower bool: A True or False boolean value indicating the tweet_text should be lowercased\n",
    "    \"\"\"\n",
    "    word_count = 0\n",
    "    \n",
    "    for word in tweet_text.split():\n",
    "        if word in sentiment_set:\n",
    "            word_count += 1\n",
    "    \n",
    "    return word_count #You should return a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts finished successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(count_sentiment_words(positive_words, \"this is a good good good class\", True) == 3)\n",
    "assert(count_sentiment_words(positive_words, \"this is a good\\tgood\\tgood class\", True) == 3)\n",
    "assert(count_sentiment_words(positive_words, \"this is a GOOD GOOD GOOD class\", False) == 0)\n",
    "assert(count_sentiment_words(positive_words, \"this is a GOOD GOOD good class\", False) == 1)\n",
    "assert(count_sentiment_words(positive_words, \"Python is the best programming language for data science\", True) == 1)\n",
    "assert(count_sentiment_words(negative_words, \"R is bad compared to Python ;)\", True) == 1)\n",
    "print(\"Asserts finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (1 point)\n",
    "\n",
    "For this exercise, you will write a function that takes two numbers as input and returns a string. Intuitively, this is a basic classification function for lexicon-based sentiment classification. \n",
    "\n",
    "The function should take as input parameters the the number of positive (num_pos_words) and negative (num_neg_words) words in each tweet to predict sentiment. If the number of positive words is greater than to the number of negative tweets (num_pos_words > num_neg_words), then predict **\"positive\"**. If the number of negative words is greater than the number of positive words (num_neg_words > num_pos_words), then predict **\"negative\"**. If both num_pos_words and num_neg_words are equal (num_neg_words = num_pos_words), predict **\"neutral\"**. This is known as lexicon-based classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(num_pos_words, num_neg_words):\n",
    "    \"\"\"\n",
    "    This function should return the string \"positive\", \"negative\", or \"neutral\" given\n",
    "    the input parameters, i.e., if num_pos_words is greater than num_neg_words, return \"positive\"\n",
    "    \n",
    "        Parameters:\n",
    "            - num_pos_words int: This is a count representing the number of positive words in a tweet.\n",
    "            - num_neg_words int: This is a count representing the number of negative words in a tweet.\n",
    "            \n",
    "        Return:\n",
    "            - Return a string \"positive\", \"negative\", or \"neutral\"\n",
    "    \"\"\"\n",
    "    result = ['positive', 'negative', 'neutral']\n",
    "    \n",
    "    if num_pos_words - num_neg_words > 0:\n",
    "        return result[0]\n",
    "    elif num_pos_words - num_neg_words < 0:\n",
    "        return result[1]\n",
    "    else:\n",
    "        return result[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assert finished successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(predict(2, 5) == 'negative')\n",
    "assert(predict(5, 2) == 'positive')\n",
    "assert(predict(3, 3) == 'neutral')\n",
    "print(\"Assert finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 (1 point)\n",
    "\n",
    "This exercise is similar to Exercise 3. However, instead of making a prediction, we should write a function that returns a sentiment score. Specifically, assume num_pos_words is 3 and num_neg_words is 4, the function should return -1. The idea is that the more *positive* the number, the more positive the sentiment. Likewise, the more *negative* the number, the more negative the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_score(num_pos_words, num_neg_words):\n",
    "    \"\"\"\n",
    "    This function should generate a sentiment score num_pos_words - num_neg_words.\n",
    "    \n",
    "        Parameters:\n",
    "            - num_pos_words int: This is a count representing the number of positive words in a tweet.\n",
    "            - num_neg_words int: This is a count representing the number of negative words in a tweet.\n",
    "            \n",
    "        Return:\n",
    "            - Return an integer representing the difference between positive words and negative words.\n",
    "    \"\"\"\n",
    "    sentiment_score = int(num_pos_words - num_neg_words)\n",
    "    \n",
    "    return sentiment_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts finished successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(predict_score(3, 1) == 2)\n",
    "assert(predict_score(2, 2) == 0)\n",
    "assert(predict_score(2, 5) == -3)\n",
    "print(\"Asserts finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 (1 point)\n",
    "\n",
    "Write a function that takes a json string as input and returns a Python object. Hint: This can be one line. You can use the json library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def json_string_to_dictionary(json_string):\n",
    "    \"\"\"\n",
    "    This function should take a json string and convert it into a Python object (hint: loads)\n",
    "    \n",
    "        Parameters:\n",
    "            - json_string str: A json string, e.g., '{\"a\": 1}'\n",
    "            \n",
    "        Returns:\n",
    "            - Return a python object, e.g., {\"a\": 1} <--- look no quotes, so this is a dictionary\n",
    "    \"\"\"\n",
    "    result = json.loads(json_string)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assert finished successfully!\n"
     ]
    }
   ],
   "source": [
    "data = json_string_to_dictionary('{\"a\": 1}')\n",
    "assert(data == {'a': 1})\n",
    "data = json_string_to_dictionary('[1,2,3]')\n",
    "assert(data == [1,2,3])\n",
    "print(\"Assert finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 (3 points)\n",
    "\n",
    "For this task, we combine the functions written for the previous exercises to classify all of the tweets in a real Twitter dataset. You should write code that does the following:\n",
    "1. Keeps track of the number of tweets\n",
    "2. Keeps track of the number of positive and negative tweets\n",
    "3. Keeps track of the user that tweets the most\n",
    "4. Keeps track of the total number of unique users\n",
    "5. Keeps track of the average number of tweets per user (how many tweets does each user make, on average)\n",
    "6. Keeps track of the most positive and negative tweets.\n",
    "\n",
    "Note: This task depends on Exercises 1 through 5. You will need to complete them first. Also, do **not** store all of the tweets in a list.  This will use too much memory because of the size of the dataset. It is okay to store all of the user's screen names.\n",
    "\n",
    "Finally, the dataset is big! So, I recommend working on a subset of the dataset to make sure your code works, i.e., you could \"break\" after the first 100 lines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number_of_tweets = 0\n",
    "total_number_of_positive_tweets = 0\n",
    "total_number_of_negative_tweets = 0\n",
    "total_number_of_users = 0\n",
    "max_tweets = 0\n",
    "user_with_most_tweets = ''\n",
    "most_positive_tweet = ''\n",
    "most_negative_tweet = ''\n",
    "average_number_tweets_per_user = 0\n",
    "unique_users = {}\n",
    "total_number_of_neutral_tweets = 0\n",
    "user_scores = {}\n",
    "max_score = 0\n",
    "min_score = 0\n",
    "\n",
    "twitter_dataset = open('puerto-rico.jsonl', 'r')\n",
    "\n",
    "for row in twitter_dataset:\n",
    "    tweet_dict = json_string_to_dictionary(row)\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    tweet_text = tweet_dict['full_text'] # MODIFY THIS LINE TO GET THE \"full_text\" from the tweet_dict\n",
    "    screen_name = tweet_dict['user']['screen_name'] # MODIFY THIS LINE TO GET THE \"screen_name\" from the tweet_dict\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    num_pos_words = count_sentiment_words(positive_words, tweet_text, True)\n",
    "    num_neg_words = count_sentiment_words(negative_words, tweet_text, True)\n",
    "    \n",
    "    sentiment_prediction = predict(num_pos_words, num_neg_words)\n",
    "    sentiment_score = predict_score(num_pos_words, num_neg_words)\n",
    "    \n",
    "    ################################\n",
    "    \n",
    "    #   1. Keep track of the number of tweets\n",
    "    total_number_of_tweets += 1\n",
    "        \n",
    "    #   2. Keep track of the number of positive and negative tweets\n",
    "    if sentiment_prediction == 'positive':\n",
    "        total_number_of_positive_tweets += 1\n",
    "    elif sentiment_prediction == 'negative':\n",
    "        total_number_of_negative_tweets += 1\n",
    "    else:\n",
    "        total_number_of_neutral_tweets += 1\n",
    "    \n",
    "    #   4. Keep track of the total number of unique users\n",
    "    unique_users[screen_name] = unique_users.get(screen_name, 0) + 1\n",
    "    \n",
    "    #   6. Keep track of the most positive and negative tweets.\n",
    "    user_scores[tweet_text] = user_scores.get(tweet_text, 0) + sentiment_score\n",
    "        \n",
    "    ################################\n",
    "\n",
    "# Total Count of Unique Users\n",
    "total_number_of_users = len(unique_users)    \n",
    "    \n",
    "#   3. Keep track of the user that tweets the most\n",
    "for k,v in unique_users.items():\n",
    "    if v > max_tweets:\n",
    "        max_tweets = v\n",
    "        user_with_most_tweets = k\n",
    "\n",
    "#   5. Keep track of the average number of tweets per user (how many tweets does each user make, on average)\n",
    "average_number_tweets_per_user = total_number_of_tweets / total_number_of_users\n",
    "        \n",
    "# Most Positive and Negative Tweets\n",
    "for k,v in user_scores.items():\n",
    "    if v > max_score:\n",
    "        max_score = v\n",
    "        most_positive_tweet = k\n",
    "    if v < min_score:\n",
    "        min_score = v\n",
    "        most_negative_tweet = k\n",
    "\n",
    "twitter_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Tweets: 737153\n",
      "Total Number of Positive Tweets: 139453\n",
      "Total Number of Negative Tweets: 173638\n",
      "Total Number of Neutral Tweets: 424062\n",
      "\n",
      "Most Positive Tweet\n",
      "RT @realDonaldTrump: It was great to have Governor @RicardoRossello of #PuertoRico🇵🇷with us at the @WhiteHouse today. We are with you! #PRS…\n",
      "\n",
      "Most Negative Tweet\n",
      "RT @EduSamani: IMPORTANT UPDATES ABOUT PUERTO RICO: Hundreds of nurses are pouring in to help and they are reporting terrible conditions ac…\n",
      "\n",
      "Total Number of Users: 286975\n",
      "Average Number of Tweets per User: 2.5687011063681506\n",
      "User with the most tweets: Noti_PuertoRico\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Number of Tweets: {}\".format(total_number_of_tweets))\n",
    "print(\"Total Number of Positive Tweets: {}\".format(total_number_of_positive_tweets))\n",
    "print(\"Total Number of Negative Tweets: {}\".format(total_number_of_negative_tweets))\n",
    "print(\"Total Number of Neutral Tweets: {}\\n\".format(total_number_of_neutral_tweets))\n",
    "\n",
    "print(\"Most Positive Tweet\")\n",
    "print(most_positive_tweet)\n",
    "print()\n",
    "\n",
    "print(\"Most Negative Tweet\")\n",
    "print(most_negative_tweet)\n",
    "print()\n",
    "\n",
    "print(\"Total Number of Users: {}\".format(total_number_of_users))\n",
    "print(\"Average Number of Tweets per User: {}\".format(average_number_tweets_per_user))\n",
    "print(\"User with the most tweets: {}\".format(user_with_most_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assert finished successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(isinstance(total_number_of_tweets, int) or isinstance(total_number_of_tweets, float))\n",
    "assert(isinstance(total_number_of_positive_tweets, int) or isinstance(total_number_of_positive_tweets, float))\n",
    "assert(isinstance(total_number_of_negative_tweets, int) or isinstance(total_number_of_negative_tweets, float))\n",
    "assert(isinstance(total_number_of_neutral_tweets, int) or isinstance(total_number_of_neutral_tweets, float))\n",
    "assert(isinstance(most_positive_tweet, str))\n",
    "assert(isinstance(most_negative_tweet, str))\n",
    "assert(isinstance(user_with_most_tweets, str))\n",
    "assert(total_number_of_tweets == 737153)\n",
    "print(\"Assert finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7 (2 points)\n",
    "\n",
    "For this exercise, you will perform manual analysis of the predictions. Modify the code to load the tweet text, then answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1: RT @TheSWPrincess: @bri_sacks To find out how to help, visit the site below. Virgin Islanders are not getting the media attention that #Pue…\n",
      "Tweet 1 Prediction: neutral\n",
      "\n",
      "Tweet 2: I have yet to be able to express my thoughts without expletives about @realDonaldTrump + Hurricane Maria recovery #PuertoRico\n",
      "Tweet 2 Prediction: positive\n",
      "\n",
      "Tweet 3: RT @TalbertSwan: @TaIbertSwan @realDonaldTrump “Sire, the people don’t have power, food, or water!” \n",
      "\n",
      "#Trump: “Let them eat paper towels!”…\n",
      "Tweet 3 Prediction: neutral\n",
      "\n",
      "Tweet 4: RT @NYPDSpecialops: #NYPD ESU K9 “Harley” &amp; “Nash” deployed as part @fema NY-TF1 have been hard at work assisting in the #PuertoRico rescue…\n",
      "Tweet 4 Prediction: neutral\n",
      "\n",
      "Tweet 5: RT @StarrMSS: .@elvisduran gave 30K to @Bethenny to charter  plane to bring supplies to #PuertoRico #HurricaneMaria. He also gave 100K to @…\n",
      "Tweet 5 Prediction: neutral\n",
      "\n",
      "Tweet 6: RT @ericbolling: When will @realDonaldTrump catch a break from fake news outrage? Very unfair slams over #PuertoRico visit.\n",
      "Tweet 6 Prediction: negative\n",
      "\n",
      "Tweet 7: FCC approves up to $77 million to restore communications after hurricane https://t.co/hn0WqJiE9T #WonkAmerica https://t.co/m6P6RvDkZi\n",
      "Tweet 7 Prediction: neutral\n",
      "\n",
      "Tweet 8: \"@daddy_yankee,#PuertoRico native,to #Donate $250,000 to #Habitat &amp; raise $1.5+ #Million!\"❤\n",
      "\n",
      "https://t.co/32kjy93dNZ\n",
      "https://t.co/15bza8gjW0\n",
      "Tweet 8 Prediction: neutral\n",
      "\n",
      "Tweet 9: RT @ericbolling: When will @realDonaldTrump catch a break from fake news outrage? Very unfair slams over #PuertoRico visit.\n",
      "Tweet 9 Prediction: negative\n",
      "\n",
      "Tweet 10: RT @chefjoseandres: Forget politics forget pundits. What I have seen in #PuertoRico is people coming together, sacrificing 2 serve. This is…\n",
      "Tweet 10 Prediction: neutral\n",
      "\n",
      "Tweet 11: RT @mercycorps: Our neighbors in #PuertoRico are resilient, but they need our help to recover + rebuild. We invite you to join us.\n",
      "\n",
      "https:/…\n",
      "Tweet 11 Prediction: positive\n",
      "\n",
      "Tweet 12: RT @StopTrump2020: At least 34 dead - #Trump blames #PuertoRico for #FEMA not having enough money.  #SAD! https://t.co/DHuW7xGlOY\n",
      "Tweet 12 Prediction: neutral\n",
      "\n",
      "Tweet 13: RT @SamaritansPurse: With your support, our disaster response team continues to bring emergency relief to families in #PuertoRico. https://…\n",
      "Tweet 13 Prediction: negative\n",
      "\n",
      "Tweet 14: RT @usairforce: 4 @USARMY Pave Hawks, 4 pallets of search &amp; rescue gear, 1 ATV and 39 search &amp; rescue passengers aboard a C5 headed to #Pue…\n",
      "Tweet 14 Prediction: neutral\n",
      "\n",
      "Tweet 15: RT @RoseAnnDeMoro: RNRN and @AFLCIO send 300+ volunteers to #PuertoRico: union nurses, construction and transportation workers fly out toda…\n",
      "Tweet 15 Prediction: neutral\n",
      "\n",
      "Tweet 16: RT @daddy_yankee: I know the reconstruction of my home island will requiere long-term solutions. - go to the link and help me raise more mo…\n",
      "Tweet 16 Prediction: neutral\n",
      "\n",
      "Tweet 17: RT @Jenniffer2012: Thanks from my heart to @FLOTUS for your caring and your commitment to help. #PuertoRico 🇺🇸🇵🇷 https://t.co/p8fkXfKbXd\n",
      "Tweet 17 Prediction: positive\n",
      "\n",
      "Tweet 18: RT @RichardMadan: Here is President Trump tossing paper towels at hurricane victims in #PuertoRico https://t.co/JjLMRNFcAt\n",
      "Tweet 18 Prediction: neutral\n",
      "\n",
      "Tweet 19: RT @JimmyPatronis: I’m deploying law enforcement assets to join @fdlepio, @FLHSMV and @MyFWC to help in #PuertoRico after #Maria: https://t…\n",
      "Tweet 19 Prediction: neutral\n",
      "\n",
      "Tweet 20: RT @ExDemLatina: .@CarmenYulinCruz is a Lying policial Corrupt hack! \n",
      "She has time to make another shirt for media rounds. #PuertoRico #San…\n",
      "Tweet 20 Prediction: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "twitter_dataset = open('puerto-rico.jsonl', 'r')\n",
    "\n",
    "num_tweets_to_print = 20\n",
    "\n",
    "num_tweets = 0\n",
    "\n",
    "for row in twitter_dataset:\n",
    "    num_tweets += 1\n",
    "    tweet_dict = json_string_to_dictionary(row)\n",
    "    \n",
    "    ###############################\n",
    "    # YOUR CODE HERE\n",
    "    tweet_text = tweet_dict['full_text'] # MODIFY THIS LINE TO GET THE \"full_text\" from the tweet_dict    \n",
    "    ###############################\n",
    "    \n",
    "    num_pos_words = count_sentiment_words(positive_words, tweet_text, True)\n",
    "    num_neg_words = count_sentiment_words(negative_words, tweet_text, True)\n",
    "    \n",
    "    sentiment_prediction = predict(num_pos_words, num_neg_words)\n",
    "    \n",
    "    print(\"Tweet {}: {}\".format(num_tweets, tweet_text))\n",
    "    print(\"Tweet {} Prediction: {}\".format(num_tweets, sentiment_prediction))\n",
    "    print()\n",
    "    \n",
    "    if num_tweets == num_tweets_to_print:\n",
    "        break\n",
    "    \n",
    "twitter_dataset.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following tasks:\n",
    " \n",
    "- Manually annotate all of the tweets printed above:\n",
    "   1. Tweet 1 Annotation Here: **Negative** / different\n",
    "   1. Tweet 2 Annotation Here: **Negative** / different\n",
    "   1. Tweet 3 Annotation Here: **Negative** / different\n",
    "   1. Tweet 4 Annotation Here: **Positive** / different\n",
    "   1. Tweet 5 Annotation Here: **Neutral**\n",
    "   1. Tweet 6 Annotation Here: **Negative**\n",
    "   1. Tweet 7 Annotation Here: **Neutral**\n",
    "   1. Tweet 8 Annotation Here: **Positive** / different\n",
    "   1. Tweet 9 Annotation Here: **Negative**\n",
    "   1. Tweet 10 Annotation Here: **Positive** / different\n",
    "   1. Tweet 11 Annotation Here: **Positive**\n",
    "   1. Tweet 12 Annotation Here: **Negative** / different\n",
    "   1. Tweet 13 Annotation Here: **Positive** / different\n",
    "   1. Tweet 14 Annotation Here: **Neutral**\n",
    "   1. Tweet 15 Annotation Here: **Neutral**\n",
    "   1. Tweet 16 Annotation Here: **Neutral**\n",
    "   1. Tweet 17 Annotation Here: **Positive**\n",
    "   1. Tweet 18 Annotation Here: **Negative** / different\n",
    "   1. Tweet 19 Annotation Here: **Neutral**\n",
    "   1. Tweet 20 Annotation Here: **Negative** / different\n",
    "\n",
    "- How many of the predictions are right or wrong compared to your annotations?\n",
    "    \n",
    "    **Compared to my annotations. 50% of the predictions were wrong (10 out of 20).**\n",
    "    \n",
    "- Do you see any major limitations of lexicon-based classificaiton (i.e., making sentiment predictions using individual words)? Use your intuition, I will accept most answers, as long as it makes some sense. Please describe and provide examples below:\n",
    "\n",
    "    **Making predictions using individual words can lead to innacurate categorization on the sentiment of phrases stripped out of twitter posts. The efficacy of single words needs to be enhanced so that the context of the text being analyzed is better assessed by the Python program. An example of a way to improve the efficacy of the lexicon is to use words that are more specific to the data being analyzed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit (2 points)\n",
    "\n",
    "For this exercise, you should use a different dataset (email me if you want me to share a dataset with your directly, or you can use your own---see below for dataset resources) and analyze it with a different (non-sentiment) lexicon. You can complete the same analysis as above, or do something different as long as you make use of a new lexicon and a new dataset. Notice some lexicons only have one class, or more than 2 classes. The processing will be slightly different. Feel free to ask how this can be done via Slack.\n",
    "\n",
    "Possible lexicons:\n",
    "- Hate speech and offensive language lexicons:\n",
    "    - https://github.com/steve050798/hate-speech-and-offensive-language/raw/master/lexicons/hatebase_dict.csv\n",
    "    - https://www.cs.cmu.edu/~biglou/resources/bad-words.txt\n",
    "- Mental (Health) Lexicons    \n",
    "    - Anxiety Lexicon\n",
    "        - https://github.com/lrheault/anxiety\n",
    "    - Depression Lexicon\n",
    "        - https://github.com/halolimat/Social-media-Depression-Detector\n",
    "    - Abuse Lexicon\n",
    "        - https://github.com/uds-lsv/lexicon-of-abusive-words\n",
    "        \n",
    "You may use one of the lexicons above or another non-sentiment lexicon of your choice. I may allow another sentiment lexicon if the new analysis is interesting; however, email me before doing this. An example would be using a Spanish sentiment lexicon to analyze Spanish text, or doing a different analysis than the one abouve (e.g., anayzing sentiment over time).\n",
    "\n",
    "Where to look for new data?\n",
    "- https://www.kaggle.com\n",
    "- https://www.docnow.io/catalog/ (Some datasets contain the text, not just tweet ids). If you have something specific you want, I can grab it for you using my developer account.\n",
    "- Google :). You do not need to use tweets.\n",
    "\n",
    "You can pull new data from Twitter, but you will need to create a developer account. The easiest way to pull new data is via the use of Twarc (https://github.com/DocNow/twarc).\n",
    "\n",
    "Finally, when you submit, I don't need to have your data, just make sure to provide a couple of examples, a link to the data (if available), and make sure all of the output of your code is printed with the output so I can analyze it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Company News Article Headings Scraped from Finviz.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Scrape Finviz for AAPL News Article Headings and Create a Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages\n",
    "import pandas as pd\n",
    "import csv\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "stock = 'AAPL'\n",
    "finviz_url = f'https://finviz.com/quote.ashx?t={stock}'\n",
    "\n",
    "req = Request(url = finviz_url, headers = {'user-agent': 'my-analysis'})\n",
    "response = urlopen(req)\n",
    "html = BeautifulSoup(response, 'html')\n",
    "\n",
    "news_tables = {}\n",
    "news_table = html.find(id = 'news-table')\n",
    "news_tables[stock] = news_table\n",
    "\n",
    "stock_data = news_tables[stock]\n",
    "stock_rows = stock_data.findAll('tr')\n",
    "\n",
    "parsed_data = []\n",
    "\n",
    "for stock, news_table in news_tables.items():\n",
    "    for row in news_table.findAll('tr'):   \n",
    "        title = row.a.text\n",
    "        date_data = row.td.text.split(' ')\n",
    "        \n",
    "        if len(date_data) == 1:\n",
    "            time = date_data[0]\n",
    "        else:\n",
    "            date = date_data[0]\n",
    "            time = date_data[1]\n",
    "        \n",
    "        parsed_data.append([stock, date, time, title])\n",
    "\n",
    "news_df = pd.DataFrame(parsed_data, columns = ['stock', 'date', 'time', 'heading'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>heading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Mar-12-21</td>\n",
       "      <td>06:02PM</td>\n",
       "      <td>These Are The Best Robinhood Stocks To Buy Or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Mar-12-21</td>\n",
       "      <td>05:45PM</td>\n",
       "      <td>Apple (AAPL) Stock Sinks As Market Gains: What...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Mar-12-21</td>\n",
       "      <td>05:45PM</td>\n",
       "      <td>How to Play the Valuation Gap Between Growth a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Mar-12-21</td>\n",
       "      <td>03:20PM</td>\n",
       "      <td>Apple is a force for good, Facebook not so muc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Mar-12-21</td>\n",
       "      <td>02:50PM</td>\n",
       "      <td>Is Apple Stock A Buy Ahead Of Possible Spring ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stock       date       time  \\\n",
       "0  AAPL  Mar-12-21  06:02PM     \n",
       "1  AAPL  Mar-12-21  05:45PM     \n",
       "2  AAPL  Mar-12-21  05:45PM     \n",
       "3  AAPL  Mar-12-21  03:20PM     \n",
       "4  AAPL  Mar-12-21  02:50PM     \n",
       "\n",
       "                                             heading  \n",
       "0  These Are The Best Robinhood Stocks To Buy Or ...  \n",
       "1  Apple (AAPL) Stock Sinks As Market Gains: What...  \n",
       "2  How to Play the Valuation Gap Between Growth a...  \n",
       "3  Apple is a force for good, Facebook not so muc...  \n",
       "4  Is Apple Stock A Buy Ahead Of Possible Spring ...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View first four rows\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Perform Sentiment Analysis with Vader Lexicon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "lambda_func = lambda title: vader.polarity_scores(title)['compound']\n",
    "\n",
    "news_df['compound score'] = news_df['heading'].apply(lambda_func)\n",
    "news_df['date'] = pd.to_datetime(news_df.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>heading</th>\n",
       "      <th>compound score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021-03-12</td>\n",
       "      <td>06:02PM</td>\n",
       "      <td>These Are The Best Robinhood Stocks To Buy Or ...</td>\n",
       "      <td>0.6369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021-03-12</td>\n",
       "      <td>05:45PM</td>\n",
       "      <td>Apple (AAPL) Stock Sinks As Market Gains: What...</td>\n",
       "      <td>0.3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021-03-12</td>\n",
       "      <td>05:45PM</td>\n",
       "      <td>How to Play the Valuation Gap Between Growth a...</td>\n",
       "      <td>0.7506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021-03-12</td>\n",
       "      <td>03:20PM</td>\n",
       "      <td>Apple is a force for good, Facebook not so muc...</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021-03-12</td>\n",
       "      <td>02:50PM</td>\n",
       "      <td>Is Apple Stock A Buy Ahead Of Possible Spring ...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stock        date       time  \\\n",
       "0  AAPL  2021-03-12  06:02PM     \n",
       "1  AAPL  2021-03-12  05:45PM     \n",
       "2  AAPL  2021-03-12  05:45PM     \n",
       "3  AAPL  2021-03-12  03:20PM     \n",
       "4  AAPL  2021-03-12  02:50PM     \n",
       "\n",
       "                                             heading  compound score  \n",
       "0  These Are The Best Robinhood Stocks To Buy Or ...          0.6369  \n",
       "1  Apple (AAPL) Stock Sinks As Market Gains: What...          0.3400  \n",
       "2  How to Play the Valuation Gap Between Growth a...          0.7506  \n",
       "3  Apple is a force for good, Facebook not so muc...          0.4404  \n",
       "4  Is Apple Stock A Buy Ahead Of Possible Spring ...          0.0000  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View first four rows\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average_comp_score for AAPL is 0.076643\n"
     ]
    }
   ],
   "source": [
    "#Average Compound Score \n",
    "score_count = 0\n",
    "comp_score = 0\n",
    "\n",
    "for score in news_df['compound score']:\n",
    "    comp_score += score\n",
    "    score_count += 1\n",
    "\n",
    "average_comp_score = comp_score / score_count\n",
    "print(f\"The average_comp_score for {stock} is {average_comp_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **What This Means**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per the Vader documentationm, the compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1(most extreme negative) and +1 (most extreme positive). In the case above, the average compound score is 0.076643, meaning that AAPL is leaning towards having positive sentiment based on the text within the Finviz news article headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Package\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mean_df = news_df.groupby(['date']).mean()\n",
    "plt.plot(mean_df, color = 'blue')\n",
    "plt.title(f\"{stock} Mean Compound Score Over Time\")\n",
    "plt.axhline(0, color = 'red')\n",
    "plt.savefig(f'08. {stock} Compund Score.png',format='png', dpi = 200, bbox_inches = 'tight', orientation = 'landscape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
