{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3\n",
    "\n",
    "For this problem set, you will expand on PS2 to perform and evaluate various sentiment classification methods.\n",
    "\n",
    "Your name: Rudy Martinez\n",
    "\n",
    "You abc123: Lpe538\n",
    "\n",
    "## Submission Instructions\n",
    "\n",
    "After completing the exercises below, generate a pdf of the code **with** outputs. After that create a zip file containing both the completed exercise and the generated PDF/HTML. You are **required** to check the PDF/HTML to make sure all the code **and** outputs are clearly visible and easy to read. If your code goes off the page, you should reduce the line size. I generally recommend not going over 80 characters.\n",
    "\n",
    "Finally, name the zip file using a combination of your the assigment and your name, e.g., ps3_rios.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (1 point)\n",
    "\n",
    "For this step, you will load the training and test sentiment datasets \"twitdata_TEST.tsv\" and \"allTrainingData.tsv\". The data should be loaded into 4 lists of strings: X_txt_train, X_txt_test, y_test, y_train.\n",
    "\n",
    "Note, when using csvreader, you need to pass the \"quoting\" the value csv.QUOTE_NONE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "X_txt_train = []\n",
    "y_train = []\n",
    "\n",
    "# Loading data from CSVs.\n",
    "with open('allTrainingData.tsv') as x_train_file:\n",
    "    tsv_reader_1 = csv.reader(x_train_file, delimiter = '\\t', quoting = csv.QUOTE_NONE)\n",
    "    \n",
    "    for row in tsv_reader_1:\n",
    "        X_txt_train.append(row)\n",
    "\n",
    "with open('allTrainingData.tsv') as y_train_file:\n",
    "    tsv_reader_2 = csv.reader(y_train_file, delimiter = '\\t', quoting = csv.QUOTE_NONE)\n",
    "    \n",
    "    for row in tsv_reader_2:\n",
    "        y_train.append(row)\n",
    "    \n",
    "# 1. Load the training datasets into two lists (X_txt_train will be a list of strings; y_train)\n",
    "\n",
    "X_txt_test = []\n",
    "y_test = []\n",
    "\n",
    "# 2. Load the test datasets into two lists (X_txt_test will be a list of strings; y_test)\n",
    "with open('twitdata_TEST.tsv') as x_test_file:\n",
    "    tsv_reader_3 = csv.reader(x_test_file, delimiter = '\\t', quoting = csv.QUOTE_NONE)\n",
    "    \n",
    "    for row in tsv_reader_3:\n",
    "        X_txt_test.append(row)\n",
    "\n",
    "with open('twitdata_TEST.tsv') as y_test_file:\n",
    "    tsv_reader_4 = csv.reader(y_test_file, delimiter = '\\t', quoting = csv.QUOTE_NONE)\n",
    "    \n",
    "    for row in tsv_reader_4:\n",
    "        y_test.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7250f9d93c31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_txt_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_txt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_txt_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_txt_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert(type(X_txt_train) == type(list()))\n",
    "assert(type(X_txt_train[0]) == type(str()))\n",
    "assert(type(X_txt_test) == type(list()))\n",
    "assert(type(X_txt_test[0]) == type(str()))\n",
    "assert(type(y_test) == type(list()))\n",
    "assert(type(y_train) == type(list()))\n",
    "assert(len(X_txt_test) == 3199)\n",
    "assert(len(y_test) == 3199)\n",
    "assert(len(X_txt_train) == 8018)\n",
    "assert(len(y_train) == 8018)\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (2 point)\n",
    "\n",
    "This part is similar to HW2 (using the positive_words and negative_words variables). We will compare last homework's lexicon-based classification method with supervised models. Only make predictions on the test split and store all predictions in the list lex_test_preds. Next, calculate the **micro** precision, recall, and f1 scores using the lex_test_preds list.\n",
    "\n",
    "You can learn more about lexicon-based classification in Chapter 19.6. If you are interested, the chapter is available online for free at the following link: [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/19.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THE CODE IN THIS CELL\n",
    "class LexiconClassifier():\n",
    "    def __init__(self):\n",
    "        self.positive_words = set()\n",
    "        with open('positive-words.txt', encoding = 'utf-8') as iFile:\n",
    "            for row in iFile:\n",
    "                self.positive_words.add(row.strip())\n",
    "\n",
    "        self.negative_words = set()\n",
    "        with open('negative-words.txt', encoding='iso-8859-1') as iFile:\n",
    "            for row in iFile:\n",
    "                self.negative_words.add(row.strip())\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        num_pos_words = 0\n",
    "        num_neg_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.positive_words:\n",
    "                num_pos_words += 1\n",
    "            elif word in self.negative_words:\n",
    "                num_neg_words += 1\n",
    "        \n",
    "        pred = 'neutral'        \n",
    "        if num_pos_words > num_neg_words:\n",
    "            pred = 'positive'\n",
    "        elif num_pos_words < num_neg_words:\n",
    "            pred = 'negative'\n",
    "            \n",
    "        return pred\n",
    "    \n",
    "    def count_pos_words(self, sentence):\n",
    "        num_pos_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.positive_words:\n",
    "                num_pos_words += 1\n",
    "        return num_pos_words\n",
    "\n",
    "    def count_neg_words(self, sentence):\n",
    "        num_neg_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.negative_words:\n",
    "                num_neg_words += 1\n",
    "        return num_neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# 1. Instatiate that class\n",
    "\n",
    "lex_test_preds = ... # Initialize this as an empty list\n",
    "\n",
    "# Loop over X_txt_test\n",
    "#    for each string in X_txt_test (i.e., for each item in the list), pass it to LexiconClassifiers .predict() method\n",
    "#    append the prediction to lex_test_preds\n",
    "\n",
    "precision = ... # Get scores using lex_test_preds and y_test with the precision_score method\n",
    "recall = ... # Get scores using lex_test_preds and y_test with the recall_score method\n",
    "f1 = ... # Get scores using lex_test_preds and y_test with the f1_score method\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(type(lex_test_preds) == type(list()))\n",
    "assert(type(lex_test_preds[0]) == type(str()))\n",
    "assert(set(lex_test_preds) == set([\"positive\", \"negative\", \"neutral\"]))\n",
    "assert(len(lex_test_preds) == len(y_test))\n",
    "assert(type(precision) == type(float()) or type(precision) == type(np.float64()))\n",
    "assert(type(recall) == type(float()) or type(recall) == type(np.float64()))\n",
    "assert(type(f1) == type(float()) or type(f1) == type(np.float64()))\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (1 point)\n",
    "\n",
    "Again, using the LexiconClassifier, write code to generate a lists of lists where each sublist contains the number of positive words and negative words in a tweet. Assume we are give the train test datasets\n",
    "\n",
    "``` python\n",
    "X_txt_train = [\"good good\", \"bad bad\"]\n",
    "X_txt_test = [\"great\", \"bad bad great\"]\n",
    "```\n",
    "\n",
    "you should write code that creates two lists of lists as follows:\n",
    "\n",
    "``` python\n",
    "X_train_lexicon_features = [[2, 0], [0,2]] # [2, 0] means the first tweeta has 2 positive words and 0 negative words.\n",
    "X_test_lexicon_features = [[1, 0], [1, 2]]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE\n",
    "\n",
    "X_train_lexicon_features = ... # Initailze to an empty list. This will be a list of lists\n",
    "X_test_lexicon_features = ... #  Initailze to an empty list. This will be a list of lists\n",
    "\n",
    "# Loop over X_txt_test\n",
    "#    for each string in X_txt_test (i.e., for each item in the list), pass it to LexiconClassifiers .count_pos_words() and count_neg_words method\n",
    "#    append a list with the counts to X_test_lexicon_features\n",
    "\n",
    "# Loop over X_txt_train\n",
    "#    for each string in X_txt_train (i.e., for each item in the list), pass it to LexiconClassifiers .count_pos_words() and count_neg_words method\n",
    "#    append a list with the counts to X_train_lexicon_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(type(X_train_lexicon_features) == type(list()))\n",
    "assert(type(X_test_lexicon_features) == type(list()))\n",
    "assert(type(X_test_lexicon_features[0]) == type(list()))\n",
    "assert(len(X_train_lexicon_features) == len(X_txt_train))\n",
    "assert(len(X_test_lexicon_features) == len(X_txt_test))\n",
    "assert(len(X_train_lexicon_features[0]) == 2)\n",
    "assert(len(X_test_lexicon_features[0]) == 2)\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 (2 points)\n",
    "\n",
    "For this task you should creat a feature matrix using CountVectorizer and train a LinearSVC model from scikit-learn. On the train split, use GridSearchCV to find the best LinearSVC C values (0.0001, 0.001, 0.001, 0.01, 0.1, 1, 10, or 100) based on the micro f1 scoring metric (hint: \"micro\" average) and set the cv parameter to 5. Also, with the CountVectorizer, only use unigrams (i.e., set ngram_range = (1,1)). Note that GridSearchCV will retrain the final classifier using the best parameters, so you don't need to do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# WRITE CODE HERE\n",
    "\n",
    "# Summary:\n",
    "# 1. Convert X_txt_train and X_txt_test to matricies of numbers (i.e., use CountVectorizer)\n",
    "\n",
    "X_train = ... # This should be a matrix\n",
    "X_test = ... # This should be a matrix\n",
    "\n",
    "# Initialize the classifier LinearSVC \n",
    "\n",
    "# Create the params with the C values\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "\n",
    "# \"fit\" the model  on X_train\n",
    "\n",
    "validation_score = ... # Get the score from the GridSearchCV \"best score\"\n",
    "print(\"Validation F1: {:.4f}\".format(validation_score))\n",
    "\n",
    "svm_test_predictions = ... # \"predict\" on X_test \n",
    "\n",
    "precision = ... # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "recall = ...\n",
    "f1 = ...\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "assert(type(X_train) == type(csr_matrix(0)) or type(X_train) == type(np.array(0)))\n",
    "assert(type(X_test) == type(csr_matrix(0)) or type(X_test) == type(np.array(0)))\n",
    "assert(X_train.shape[0] == len(X_txt_train))\n",
    "assert(X_test.shape[0] == len(X_txt_test))\n",
    "assert(X_train.shape[1] == X_test.shape[1])\n",
    "assert(type(precision) == type(float()) or type(precision) == type(np.float64()))\n",
    "assert(type(recall) == type(float()) or type(recall) == type(np.float64()))\n",
    "assert(type(f1) == type(float()) or type(f1) == type(np.float64()))\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 (2 points)\n",
    "\n",
    "Repeat the experiment from exercise 4, but include the lexicon features with the CountVectorizer features. Specifically, you need to concatenate the variables ```X_train_lexicon_features``` and ```X_test_lexicon_features``` with ```X_train``` and ```X_test```, respectively. Intuitively, we are performing feature engineering by adding \"lexicon features\".\n",
    "\n",
    "HINT: You will need to convert the lexicon features to numpy arrays then call hstack from the scipy.sparse library (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.hstack.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# WRITE CODE HERE\n",
    "\n",
    "# Summary:\n",
    "# 1. Convert X_txt_train and X_txt_test to matricies of numbers (i.e., use CountVectorizer)\n",
    "\n",
    "X_train_w_lex = ... # This will be the matrix from CountVectorizer (X_txt_train)\n",
    "X_test_w_lex = ...\n",
    "\n",
    "# Now we need to convert X_train_lexicon_features and X_test_lexicon_features to numpy arrays\n",
    "# \"hstack\" X_train_lexicon_features with X_train_w_lex\n",
    "# \"hstack\" X_test_lexicon_features with X_test_w_lex\n",
    "\n",
    "# Initialize the classifier LinearSVC \n",
    "\n",
    "# Create the params with the C values\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "\n",
    "# \"fit\" the model  on X_train_w_lex\n",
    "\n",
    "\n",
    "validation_score = ...\n",
    "print(\"Validation F1: {:.4f}\".format(validation_score))\n",
    "\n",
    "svm_lex_test_predictions = ... # Get predictions on X_test_w_lex\n",
    "\n",
    "precision = ... # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "recall = ...\n",
    "f1 = ...\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "assert(X_train_w_lex.shape[0] == len(X_txt_train))\n",
    "assert(X_test.shape[0] == len(X_txt_test))\n",
    "assert(X_train_w_lex.shape[1] == X_test.shape[1] + 2)\n",
    "assert(X_train_w_lex.shape[1] == X_test_w_lex.shape[1])\n",
    "assert(type(precision) == type(float()) or type(precision) == type(np.float64()))\n",
    "assert(type(recall) == type(float()) or type(recall) == type(np.float64()))\n",
    "assert(type(f1) == type(float()) or type(f1) == type(np.float64()))\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 (2 points)\n",
    "\n",
    "For this exercise, you will perform manual analysis of the predictions. Answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tweets = 0\n",
    "for text, svm_pred, svm_lex_pred, lex_pred, y  in zip(X_txt_test, svm_test_predictions, svm_lex_test_predictions, lex_test_preds, y_test):\n",
    "    print(\"Tweet: {}\".format(text))\n",
    "    print(\"Ground-Truth Class: {}\".format(y))\n",
    "    print(\"SVM Prediction: {}\".format(svm_pred))\n",
    "    print(\"SVM+Lexicon Prediction: {}\".format(svm_lex_pred))\n",
    "    print(\"Lexicon Model Prediction: {}\".format(lex_pred))\n",
    "    print()\n",
    "    \n",
    "    num_tweets += 1\n",
    "    if num_tweets == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following tasks:\n",
    " \n",
    "- Manually annotate all of the tweets printed above:\n",
    "   1. Tweet 1 Annotation Here\n",
    "   1. Tweet 2 Annotation Here\n",
    "   1. Tweet 3 Annotation Here\n",
    "   1. Tweet 4 Annotation Here\n",
    "   1. Tweet 5 Annotation Here\n",
    "   1. Tweet 6 Annotation Here\n",
    "   1. Tweet 7 Annotation Here\n",
    "   1. Tweet 8 Annotation Here\n",
    "   1. Tweet 9 Annotation Here\n",
    "   1. Tweet 10 Annotation Here\n",
    "   1. Tweet 11 Annotation Here\n",
    "   1. Tweet 12 Annotation Here\n",
    "   1. Tweet 13 Annotation Here\n",
    "   1. Tweet 14 Annotation Here\n",
    "   1. Tweet 15 Annotation Here\n",
    "   1. Tweet 16 Annotation Here\n",
    "   1. Tweet 17 Annotation Here\n",
    "   1. Tweet 18 Annotation Here\n",
    "   1. Tweet 19 Annotation Here\n",
    "   1. Tweet 20 Annotation Here\n",
    "\n",
    "- How many of your annotations match the ground truth labels? Do you think the datasets labels are correct? (Use your intuition)\n",
    "    - Answer here\n",
    "\n",
    "- How many of your annotations match the lexicon-based model's predictions?\n",
    "    - Answer here\n",
    "\n",
    "- How many of your annotations match the SVM's predictions?\n",
    "    - Answer here\n",
    "    \n",
    "- How many of your annotations match the SVM+Lexicon's predictions?\n",
    "    - Answer here\n",
    "    \n",
    "- Do you see any major limitations of the linear SVM model? Use your intuition, I will accept most answers, as long as it makes some sense. Please describe and provide examples below:\n",
    "\n",
    "Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit 1 (2 points)\n",
    "\n",
    "For this extra credit the only goal is to improve your model on the test set (i.e., increase the micro f1 score). You may create new features, grid search over more parameters, try different feature weighting methods (e.g., TfidfVectorizer), or test different machine learning models. You can do whatever you want as long as the final test score improves, I will provide you with the extra credit points.\n",
    "\n",
    "DO NOT TRAIN ON THE TEST SET. That is cheating!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
