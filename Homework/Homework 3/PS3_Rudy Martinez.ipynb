{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3\n",
    "\n",
    "For this problem set, you will expand on PS2 to perform and evaluate various sentiment classification methods.\n",
    "\n",
    "Your name: Rudy Martinez\n",
    "\n",
    "You abc123: Lpe538\n",
    "\n",
    "## Submission Instructions\n",
    "\n",
    "After completing the exercises below, generate a pdf of the code **with** outputs. After that create a zip file containing both the completed exercise and the generated PDF/HTML. You are **required** to check the PDF/HTML to make sure all the code **and** outputs are clearly visible and easy to read. If your code goes off the page, you should reduce the line size. I generally recommend not going over 80 characters.\n",
    "\n",
    "Finally, name the zip file using a combination of your the assigment and your name, e.g., ps3_rios.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (1 point)\n",
    "\n",
    "For this step, you will load the training and test sentiment datasets \"twitdata_TEST.tsv\" and \"allTrainingData.tsv\". The data should be loaded into 4 lists of strings: X_txt_train, X_txt_test, y_test, y_train.\n",
    "\n",
    "Note, when using csvreader, you need to pass the \"quoting\" the value csv.QUOTE_NONE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_txt_train = []\n",
    "y_train = []\n",
    "\n",
    "with open('allTrainingData.tsv') as x_train_file:\n",
    "    tsv_reader = csv.reader(x_train_file, delimiter = '\\t', quoting = csv.QUOTE_NONE)\n",
    "    \n",
    "    for row in tsv_reader:\n",
    "        X_txt_train.append(row[3])\n",
    "        y_train.append(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_txt_test = []\n",
    "y_test = []\n",
    "\n",
    "with open('twitdata_TEST.tsv') as x_test_file:\n",
    "    tsv_reader = csv.reader(x_test_file, delimiter = '\\t', quoting = csv.QUOTE_NONE)\n",
    "    \n",
    "    for row in tsv_reader:\n",
    "        X_txt_test.append(row[3])\n",
    "        y_test.append(row[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(type(X_txt_train) == type(list()))\n",
    "assert(type(X_txt_train[0]) == type(str()))\n",
    "assert(type(X_txt_test) == type(list()))\n",
    "assert(type(X_txt_test[0]) == type(str()))\n",
    "assert(type(y_test) == type(list()))\n",
    "assert(type(y_train) == type(list()))\n",
    "assert(len(X_txt_test) == 3199)\n",
    "assert(len(y_test) == 3199)\n",
    "assert(len(X_txt_train) == 8018)\n",
    "assert(len(y_train) == 8018)\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (2 point)\n",
    "\n",
    "This part is similar to HW2 (using the positive_words and negative_words variables). We will compare last homework's lexicon-based classification method with supervised models. Only make predictions on the test split and store all predictions in the list lex_test_preds. Next, calculate the **micro** precision, recall, and f1 scores using the lex_test_preds list.\n",
    "\n",
    "You can learn more about lexicon-based classification in Chapter 19.6. If you are interested, the chapter is available online for free at the following link: [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/19.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THE CODE IN THIS CELL\n",
    "class LexiconClassifier():\n",
    "    def __init__(self):\n",
    "        self.positive_words = set()\n",
    "        with open('positive-words.txt', encoding = 'utf-8') as iFile:\n",
    "            for row in iFile:\n",
    "                self.positive_words.add(row.strip())\n",
    "\n",
    "        self.negative_words = set()\n",
    "        with open('negative-words.txt', encoding='iso-8859-1') as iFile:\n",
    "            for row in iFile:\n",
    "                self.negative_words.add(row.strip())\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        num_pos_words = 0\n",
    "        num_neg_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.positive_words:\n",
    "                num_pos_words += 1\n",
    "            elif word in self.negative_words:\n",
    "                num_neg_words += 1\n",
    "        \n",
    "        pred = 'neutral'        \n",
    "        if num_pos_words > num_neg_words:\n",
    "            pred = 'positive'\n",
    "        elif num_pos_words < num_neg_words:\n",
    "            pred = 'negative'\n",
    "            \n",
    "        return pred\n",
    "    \n",
    "    def count_pos_words(self, sentence):\n",
    "        num_pos_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.positive_words:\n",
    "                num_pos_words += 1\n",
    "        return num_pos_words\n",
    "\n",
    "    def count_neg_words(self, sentence):\n",
    "        num_neg_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.negative_words:\n",
    "                num_neg_words += 1\n",
    "        return num_neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5827\n",
      "Recall: 0.5827\n",
      "F1: 0.5827\n"
     ]
    }
   ],
   "source": [
    "# WRITE CODE HERE\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# 1. Instatiate that class\n",
    "lex_class = LexiconClassifier()\n",
    "\n",
    "lex_test_preds = [] # Initialize this as an empty list\n",
    "\n",
    "# Loop over X_txt_test\n",
    "#    for each string in X_txt_test (i.e., for each item in the list), pass it to LexiconClassifiers .predict() method\n",
    "#    append the prediction to lex_test_preds\n",
    "for string in X_txt_test:\n",
    "    lex_test_preds.append(lex_class.predict(string))\n",
    "\n",
    "precision = precision_score(y_test, lex_test_preds, average = 'micro') # Get scores using lex_test_preds and y_test with the precision_score method\n",
    "recall = recall_score(y_test, lex_test_preds, average = 'micro') # Get scores using lex_test_preds and y_test with the recall_score method\n",
    "f1 = f1_score(y_test, lex_test_preds, average = 'micro') # Get scores using lex_test_preds and y_test with the f1_score method\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(type(lex_test_preds) == type(list()))\n",
    "assert(type(lex_test_preds[0]) == type(str()))\n",
    "assert(set(lex_test_preds) == set([\"positive\", \"negative\", \"neutral\"]))\n",
    "assert(len(lex_test_preds) == len(y_test))\n",
    "assert(type(precision) == type(float()) or type(precision) == type(np.float64()))\n",
    "assert(type(recall) == type(float()) or type(recall) == type(np.float64()))\n",
    "assert(type(f1) == type(float()) or type(f1) == type(np.float64()))\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (1 point)\n",
    "\n",
    "Again, using the LexiconClassifier, write code to generate a lists of lists where each sublist contains the number of positive words and negative words in a tweet. Assume we are give the train test datasets\n",
    "\n",
    "``` python\n",
    "X_txt_train = [\"good good\", \"bad bad\"]\n",
    "X_txt_test = [\"great\", \"bad bad great\"]\n",
    "```\n",
    "\n",
    "you should write code that creates two lists of lists as follows:\n",
    "\n",
    "``` python\n",
    "X_train_lexicon_features = [[2, 0], [0,2]] # [2, 0] means the first tweet has 2 positive words and 0 negative words.\n",
    "X_test_lexicon_features = [[1, 0], [1, 2]]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE\n",
    "\n",
    "X_train_lexicon_features = [] # Initailize to an empty list. This will be a list of lists\n",
    "X_test_lexicon_features = [] #  Initailize to an empty list. This will be a list of lists\n",
    "\n",
    "# Loop over X_txt_test\n",
    "#    for each string in X_txt_test (i.e., for each item in the list), pass it to LexiconClassifiers .count_pos_words() and count_neg_words method\n",
    "#    append a list with the counts to X_test_lexicon_features\n",
    "for string in X_txt_test:\n",
    "    X_test_lexicon_features.append([lex_class.count_pos_words(string), lex_class.count_neg_words(string)])\n",
    "\n",
    "# Loop over X_txt_train\n",
    "#    for each string in X_txt_train (i.e., for each item in the list), pass it to LexiconClassifiers .count_pos_words() and count_neg_words method\n",
    "#    append a list with the counts to X_train_lexicon_features\n",
    "for string in X_txt_train:\n",
    "    X_train_lexicon_features.append([lex_class.count_pos_words(string), lex_class.count_neg_words(string)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(type(X_train_lexicon_features) == type(list()))\n",
    "assert(type(X_test_lexicon_features) == type(list()))\n",
    "assert(type(X_test_lexicon_features[0]) == type(list()))\n",
    "assert(len(X_train_lexicon_features) == len(X_txt_train))\n",
    "assert(len(X_test_lexicon_features) == len(X_txt_test))\n",
    "assert(len(X_train_lexicon_features[0]) == 2)\n",
    "assert(len(X_test_lexicon_features[0]) == 2)\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 (2 points)\n",
    "\n",
    "For this task you should creat a feature matrix using CountVectorizer and train a LinearSVC model from scikit-learn. On the train split, use GridSearchCV to find the best LinearSVC C values (0.0001, 0.001, 0.001, 0.01, 0.1, 1, 10, or 100) based on the micro f1 scoring metric (hint: \"micro\" average) and set the cv parameter to 5. Also, with the CountVectorizer, only use unigrams (i.e., set ngram_range = (1,1)). Note that GridSearchCV will retrain the final classifier using the best parameters, so you don't need to do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1: 0.6593\n",
      "Precision: 0.6511\n",
      "Recall: 0.6511\n",
      "F1: 0.6511\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# WRITE CODE HERE\n",
    "\n",
    "# Summary:\n",
    "# 1. Convert X_txt_train and X_txt_test to matricies of numbers (i.e., use CountVectorizer)\n",
    "vec = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "X_train = vec.fit_transform(X_txt_train) # This should be a matrix\n",
    "X_test = vec.transform(X_txt_test) # This should be a matrix\n",
    "\n",
    "# Initialize the classifier LinearSVC \n",
    "svc = LinearSVC()\n",
    "\n",
    "# Create the params with the C values\n",
    "params = {\"C\": [0.0001, 0.001, 0.001, 0.01, 0.1, 1., 10., 100.]}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "clf = GridSearchCV(svc, params, cv = 5, scoring = 'f1_micro')\n",
    "\n",
    "# \"fit\" the model  on X_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "validation_score = clf.best_score_ # Get the score from the GridSearchCV \"best score\"\n",
    "print(\"Validation F1: {:.4f}\".format(validation_score))\n",
    "\n",
    "svm_test_predictions = clf.predict(X_test) # \"predict\" on X_test \n",
    "\n",
    "precision = precision_score(y_test, svm_test_predictions, average = 'micro') # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "recall = recall_score(y_test, svm_test_predictions, average = 'micro')\n",
    "f1 = f1_score(y_test, svm_test_predictions, average = 'micro')\n",
    "\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "assert(type(X_train) == type(csr_matrix(0)) or type(X_train) == type(np.array(0)))\n",
    "assert(type(X_test) == type(csr_matrix(0)) or type(X_test) == type(np.array(0)))\n",
    "assert(X_train.shape[0] == len(X_txt_train))\n",
    "assert(X_test.shape[0] == len(X_txt_test))\n",
    "assert(X_train.shape[1] == X_test.shape[1])\n",
    "assert(type(precision) == type(float()) or type(precision) == type(np.float64()))\n",
    "assert(type(recall) == type(float()) or type(recall) == type(np.float64()))\n",
    "assert(type(f1) == type(float()) or type(f1) == type(np.float64()))\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 (2 points)\n",
    "\n",
    "Repeat the experiment from exercise 4, but include the lexicon features with the CountVectorizer features. Specifically, you need to concatenate the variables ```X_train_lexicon_features``` and ```X_test_lexicon_features``` with ```X_train``` and ```X_test```, respectively. Intuitively, we are performing feature engineering by adding \"lexicon features\".\n",
    "\n",
    "HINT: You will need to convert the lexicon features to numpy arrays then call hstack from the scipy.sparse library (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.hstack.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1: 0.6729\n",
      "Precision: 0.6555\n",
      "Recall: 0.6555\n",
      "F1: 0.6555\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# WRITE CODE HERE\n",
    "\n",
    "# Summary:\n",
    "# 1. Convert X_txt_train and X_txt_test to matricies of numbers (i.e., use CountVectorizer)\n",
    "vec = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "X_train_w_lex = vec.fit_transform(X_txt_train) # This will be the matrix from CountVectorizer (X_txt_train)\n",
    "X_test_w_lex = vec.transform(X_txt_test)\n",
    "\n",
    "# Now we need to convert X_train_lexicon_features and X_test_lexicon_features to numpy arrays\n",
    "# \"hstack\" X_train_lexicon_features with X_train_w_lex\n",
    "# \"hstack\" X_test_lexicon_features with X_test_w_lex\n",
    "X_train_lexicon_features = np.array(X_train_lexicon_features)\n",
    "X_test_lexicon_features = np.array(X_test_lexicon_features)\n",
    "\n",
    "X_train_w_lex = sp.hstack((X_train_lexicon_features, X_train_w_lex))\n",
    "X_test_w_lex = sp.hstack((X_test_lexicon_features, X_test_w_lex))\n",
    "\n",
    "# Initialize the classifier LinearSVC \n",
    "svc = LinearSVC()\n",
    "\n",
    "# Create the params with the C values\n",
    "params = {\"C\": [0.0001, 0.001, 0.001, 0.01, 0.1, 1., 10., 100.]}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "clf = GridSearchCV(svc, params, cv = 5)\n",
    "\n",
    "# \"fit\" the model  on X_train_w_lex\n",
    "clf.fit(X_train_w_lex, y_train)\n",
    "\n",
    "validation_score = clf.best_score_\n",
    "print(\"Validation F1: {:.4f}\".format(validation_score))\n",
    "\n",
    "svm_lex_test_predictions = clf.predict(X_test_w_lex) # Get predictions on X_test_w_lex\n",
    "\n",
    "precision = precision_score(y_test, svm_lex_test_predictions, average = 'micro') # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "recall = recall_score(y_test, svm_lex_test_predictions, average = 'micro')\n",
    "f1 = f1_score(y_test, svm_lex_test_predictions, average = 'micro')\n",
    "\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "assert(X_train_w_lex.shape[0] == len(X_txt_train))\n",
    "assert(X_test.shape[0] == len(X_txt_test))\n",
    "assert(X_train_w_lex.shape[1] == X_test.shape[1] + 2)\n",
    "assert(X_train_w_lex.shape[1] == X_test_w_lex.shape[1])\n",
    "assert(type(precision) == type(float()) or type(precision) == type(np.float64()))\n",
    "assert(type(recall) == type(float()) or type(recall) == type(np.float64()))\n",
    "assert(type(f1) == type(float()) or type(f1) == type(np.float64()))\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 (2 points)\n",
    "\n",
    "For this exercise, you will perform manual analysis of the predictions. Answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Tweet: Musical awareness: Great Big Beautiful Tomorrow has an ending, Now is the time does not\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "2 Tweet: On Radio786 100.4fm 7:10 Fri Oct 19 Labour analyst Shawn Hattingh: Cosatu's role in the context of unrest in the mining http://t.co/46pjzzl6\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "3 Tweet: Kapan sih lo ngebuktiin,jan ngomong doang Susah Susah.usaha Aja blm udh nyerah,inget.if you never try you'll never know.cowok kok gentle bgt\n",
      "Ground-Truth Class: negative\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "4 Tweet: Tomorrow come and hear @DavidWillettsMP&amp;@MASieghart debate \"Navigating the new Higher Education market\" 5.30pm, Jurys Inn #CPC12\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "5 Tweet: Excuse the connectivity of this live stream, from Baba Amr, so many activists using only one Sat Modem. LIVE http://t.co/U283IhZ5 #Homs\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "6 Tweet: Show your LOVE for your local field &amp; it might win an award!  Gallagher Park #Bedlington current 4th in National Award http://t.co/WeiMDtQt\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "7 Tweet: @firecore Can you tell me when an update for the Apple TV 3rd gen becomes available? The missing update holds me back from buying #appletv3\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "8 Tweet: @Heavensbasement The Crown, Filthy McNastys, Katy Dalys or the Duke if York in Belfast! Can't wait to catch you guys tomorrow night!\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "9 Tweet: Uncover the Eternal City! Return flights to Rome travel on the 21st January, for 3 nights Augustea, 3 star Hotel... http://t.co/tw0Jeh9g\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "10 Tweet: My #cre blog Oklahoma Per Square Foot returns to the @JournalRecord blog hub tomorrow. I will have some interesting local data to share.\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "11 Tweet: \"@bbcburnsy: Loads from SB; talks with Chester continue; no deals 4 out of contract players 'til Jan; Dev t Roth ,Coops to Chest'ld #hcafc\"\n",
      "Ground-Truth Class: negative\n",
      "SVM Prediction: negative\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "12 Tweet: Trey Burke has been suspended for the Northern Michigan game (exhibition) tomorrow. http://t.co/oefkAElW\n",
      "Ground-Truth Class: negative\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "13 Tweet: W.O.W Wednesday!Marni lands this Lumberjack vest for the ladies looking to bring a little Tom boy toughness  http://t.co/7NyCbdJR\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "14 Tweet: Activists in Deir Ezzor captured this image of Musab Bin Umair Mosque after regime forces set it on fire Wednesday. http://t.co/MRcoprCE\n",
      "Ground-Truth Class: negative\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "15 Tweet: @karaotr You will appreciate this.. Sunday brunch coffee: Normal cup in b/g and then the BOWL of java. Yowza. http://t.co/XhbtaCvm\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "16 Tweet: Join me Wed for a live webcast on cost optimization for IT, for the SMB crowd. http://t.co/tyJn4RES  &lt;&lt; send your questions in! #DellWebcast\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "17 Tweet: Special THANKS to EVERYONE for coming out to Taboo Tuesday With DST tonight! It was FUN&amp;educational!!! :) @XiEtaDST\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "18 Tweet: @fatimasule That was the revelation I mentioned on sunday evening. I am still in Abj. How are u &amp; where have u been again?\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "19 Tweet: Kim Hyung Jun - Football Team the 2nd A Match at YeongDeungPo-gu DaeRimDong [12.10.27] Credit : tlxhah #6 http://t.co/u7mPTl0X\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "20 Tweet: The audio booth is ready to blow the roof off the Comcast Center tomorrow! Are you? #MDMadness http://t.co/B19fECgY\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_tweets = 1\n",
    "num_tweets = 0\n",
    "for text, svm_pred, svm_lex_pred, lex_pred, y  in zip(X_txt_test, svm_test_predictions, svm_lex_test_predictions, lex_test_preds, y_test):\n",
    "    print(\"{} Tweet: {}\".format(count_tweets, text))\n",
    "    print(\"Ground-Truth Class: {}\".format(y))\n",
    "    print(\"SVM Prediction: {}\".format(svm_pred))\n",
    "    print(\"SVM+Lexicon Prediction: {}\".format(svm_lex_pred))\n",
    "    print(\"Lexicon Model Prediction: {}\".format(lex_pred))\n",
    "    print()\n",
    "    \n",
    "    num_tweets += 1\n",
    "    count_tweets += 1\n",
    "    if num_tweets == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following tasks:\n",
    " \n",
    "- Manually annotate all of the tweets printed above:\n",
    "   1. Tweet 1 Annotation Here **Positive**\n",
    "   1. Tweet 2 Annotation Here **Neutral**\n",
    "   1. Tweet 3 Annotation Here **Neutral**\n",
    "   1. Tweet 4 Annotation Here **Neutral**\n",
    "   1. Tweet 5 Annotation Here **Neutral**\n",
    "   1. Tweet 6 Annotation Here **Positive**\n",
    "   1. Tweet 7 Annotation Here **Negative** \n",
    "   1. Tweet 8 Annotation Here **Positive**\n",
    "   1. Tweet 9 Annotation Here **Neutral**\n",
    "   1. Tweet 10 Annotation Here **Neutral**\n",
    "   1. Tweet 11 Annotation Here **Neutral**\n",
    "   1. Tweet 12 Annotation Here **Negative**\n",
    "   1. Tweet 13 Annotation Here **Positive**\n",
    "   1. Tweet 14 Annotation Here **Negative**\n",
    "   1. Tweet 15 Annotation Here **Positive**\n",
    "   1. Tweet 16 Annotation Here **Positive**\n",
    "   1. Tweet 17 Annotation Here **Positive**\n",
    "   1. Tweet 18 Annotation Here **Neutral** \n",
    "   1. Tweet 19 Annotation Here **Neutral**\n",
    "   1. Tweet 20 Annotation Here **Positive**\n",
    "\n",
    "- How many of your annotations match the ground truth labels? Do you think the datasets labels are correct? (Use your intuition)\n",
    "    - **15**\n",
    "\n",
    "- How many of your annotations match the lexicon-based model's predictions?\n",
    "    - **7**\n",
    "\n",
    "- How many of your annotations match the SVM's predictions?\n",
    "    - **12**\n",
    "    \n",
    "- How many of your annotations match the SVM+Lexicon's predictions?\n",
    "    - **10**\n",
    "    \n",
    "- Do you see any major limitations of the linear SVM model? Use your intuition, I will accept most answers, as long as it makes some sense. Please describe and provide examples below:\n",
    "\n",
    "Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major limitation that I notice with the linear SVM model is its inability to pick up on punctuation within the tweets that can drastically change the true sentiment around specific wording. Additionally, I noticed that the model takes some time to process and achieve a final result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
