{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Do List\n",
    "1. Everyone experiment with different models - see if we can get a high predicition score\n",
    "2. Figure out what Rios means by reporting validation results using cross validation\n",
    "3. Figure out how to write code to replace the current labels with the predictions from our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the training and test sentiment datasets \"test.tsv\" and \"train.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_train = []\n",
    "y_train = []\n",
    "\n",
    "with open('train.tsv') as file:\n",
    "    tsv_reader = csv.reader(file, delimiter = '\\t', quoting = csv.QUOTE_NONE)\n",
    "    \n",
    "    for row in tsv_reader:\n",
    "        X_text_train.append(row[1]) #text being processed\n",
    "        y_train.append(row[2]) #label\n",
    "        \n",
    "X_text_test = []\n",
    "y_test = []\n",
    "\n",
    "with open('test.tsv') as file:\n",
    "    tsv_reader = csv.reader(file, delimiter = '\\t', quoting = csv.QUOTE_NONE)\n",
    "    \n",
    "    for row in tsv_reader:\n",
    "        X_text_test.append(row[1]) #text being processed\n",
    "        y_test.append(row[2]) #label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert X_text_train and X_text_test to matricies of numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "vec = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "X_train = vec.fit_transform(X_text_train) \n",
    "X_test = vec.transform(X_text_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) LinearSVC Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the classifier LinearSVC, Create the params with the C values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC()\n",
    "\n",
    "params = {\"C\": [0.0001, 0.001, 0.001, 0.01, 0.1, 1., 10., 100.]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize GridSearchCV and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=20, estimator=LinearSVC(),\n",
       "             param_grid={'C': [0.0001, 0.001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                               100.0]},\n",
       "             scoring='f1_micro')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(svc, params, cv = 20, scoring = 'f1_micro')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the score from the GridSearchCV \"best score\" and Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1: 0.7357\n",
      "Best Params: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "validation_score = clf.best_score_ \n",
    "print(\"Validation F1: {:.4f}\".format(validation_score))\n",
    "\n",
    "best_parameters = clf.best_params_\n",
    "print(f\"Best Params: {best_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"predict\" on X_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_test_predictions = clf.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get scores using svm_test_predictions and y_test with the precision_score method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7753\n",
      "Recall: 0.7753\n",
      "F1: 0.7753\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, svm_test_predictions, average = 'micro')\n",
    "recall = recall_score(y_test, svm_test_predictions, average = 'micro')\n",
    "f1 = f1_score(y_test, svm_test_predictions, average = 'micro')\n",
    "\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store predictions and write to a new CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predictions\n",
    "svm_test_predictions_df = pd.DataFrame(data = svm_test_predictions)\n",
    "\n",
    "# Write to a csv\n",
    "with open('test.tsv') as file:\n",
    "    test_file = csv.reader(file, delimiter = '\\t', quoting = csv.QUOTE_NONE)\n",
    "    test_df = pd.DataFrame(test_file)\n",
    "    \n",
    "test_df[2] = svm_test_predictions_df[0]\n",
    "test_df.columns = ['TWITTER_ID', 'TEXT', 'PREDICTIONS']\n",
    "test_df.to_csv('00. LinearSVC Prediction Results.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the RandomForest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_forest = RandomForestClassifier()\n",
    "parameters = {'n_estimators': [10, 100, 200, 300, 400]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize GridSearchCV and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, estimator=RandomForestClassifier(),\n",
       "             param_grid={'n_estimators': [10, 100, 200, 300, 400]})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rand = GridSearchCV(rand_forest, parameters, cv = 2)\n",
    "clf_rand.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the score from the GridSearchCV \"best score\" and Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1: 0.7317\n",
      "Best Params: {'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "validation_score = clf_rand.best_score_ \n",
    "print(\"Validation F1: {:.4f}\".format(validation_score))\n",
    "\n",
    "best_parameters = clf_rand.best_params_\n",
    "print(f\"Best Params: {best_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"predict\" on X_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_test_predictions = clf_rand.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get scores using rand_forest_predictions and y_test with the precision_score method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8606\n",
      "Recall: 0.8606\n",
      "F1: 0.8606\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, rand_test_predictions, average = 'micro')\n",
    "recall = recall_score(y_test, rand_test_predictions, average = 'micro')\n",
    "f1 = f1_score(y_test, rand_test_predictions, average = 'micro')\n",
    "\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store predictions and write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predictions\n",
    "rand_test_predictions_df = pd.DataFrame(data = rand_test_predictions)\n",
    "\n",
    "# Write to a csv\n",
    "with open('test.tsv') as file:\n",
    "    test_file = csv.reader(file, delimiter = '\\t', quoting = csv.QUOTE_NONE)\n",
    "    test_df = pd.DataFrame(test_file)\n",
    "    \n",
    "test_df[2] = rand_test_predictions_df[0]\n",
    "test_df.columns = ['TWITTER_ID', 'TEXT', 'PREDICTIONS']\n",
    "test_df.to_csv('00. RandomForest Prediction Results.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
